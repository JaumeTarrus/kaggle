{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac6f29f",
   "metadata": {},
   "source": [
    "## Model nÂº2\n",
    "\n",
    "Second try at building a ML for the Titanic competition. I'm going to focus on being neater in the preprocessing as well as trying different types of models and playing with their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c72df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, compose, tree, model_selection, metrics, ensemble, neighbors, svm, naive_bayes, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd25f67",
   "metadata": {},
   "source": [
    "We load the files. I found the Survival features of the test samples on Kaggle so I can test without having to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d6abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the files. I found the Survival features of the test samples on Kaggle so I can test without having to submit.\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "results = pd.read_csv('results.csv')\n",
    "\n",
    "# Missing age data is filled with an arbitrary negative number so when later the Age is binned it will become its own bin.\n",
    "\n",
    "train.loc[pd.isna(train['Age']) == True, ['Age']] = -15\n",
    "test.loc[pd.isna(test['Age']) == True, ['Age']] = -15\n",
    "\n",
    "# Missing Fare data in a sample is filled with the average of the Pclass of the sample.\n",
    "\n",
    "test.loc[pd.isna(test['Fare']) == True, ['Fare']] =  test.groupby(['Pclass']).mean()['Fare'][test.loc[pd.isna(test['Fare']) == True]['Pclass'].iloc[0]]\n",
    "\n",
    "# Custom age binnig function\n",
    "\n",
    "def bin_values(data):\n",
    "    bins = [-20, 0, 10, 60, np.inf]\n",
    "    labels = [0, 1, 2, 3]\n",
    "    data['Age'] = pd.cut(data['Age'], bins=bins, labels=labels, retbins= False)\n",
    "    return data\n",
    "\n",
    "# Define train and test data samples\n",
    "\n",
    "x_train = train.drop('Survived', axis=1)\n",
    "y_train = train['Survived']\n",
    "#x_train, x_test, y_train, y_test = model_selection.train_test_split(x_train, y_train, test_size=0.7, random_state= 1)\n",
    "x_test = test \n",
    "y_test= results['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2368dc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8159  [Train set accuracy Tree]\n",
      "Accuracy: 0.7775  [Test set accuracy Tree]\n",
      "Accuracy: 0.8440  [Train set accuracy Grad. Boost]\n",
      "Accuracy: 0.7751  [Test set accuracy Grad. Boost]\n",
      "Accuracy: 0.7508  [Train set accuracy GaussianNB]\n",
      "Accuracy: 0.7560  [Test set accuracy GaussianNB]\n",
      "Accuracy: 0.7744  [Train set accuracy K Neighnors]\n",
      "Accuracy: 0.7560  [Test set accuracy K Neighnors]\n",
      "Accuracy: 0.8036  [Train set accuracy SVC]\n",
      "Accuracy: 0.7895  [Test set accuracy SVC]\n",
      "Accuracy: 0.8114  [Train set accuracy Random Forrest]\n",
      "Accuracy: 0.7727  [Test set accuracy Random Forrest]\n",
      "Accuracy: 0.7464  [Train set accuracy SGD]\n",
      "Accuracy: 0.7057  [Test set accuracy SGD]\n"
     ]
    }
   ],
   "source": [
    "#Cateforical data is encoded using OneHotEncoder. For SibSp and Parch we only consider to categories (i.e. family or no family).\n",
    "# Age is binned using the custom binning function, and we use KBinsDiscretizer to bin Fare.\n",
    "\n",
    "column_trans = compose.ColumnTransformer(\n",
    "    [('sex', preprocessing.OneHotEncoder(drop='if_binary', handle_unknown = 'ignore'), ['Sex']),\n",
    "    ('pclass', preprocessing.OneHotEncoder(handle_unknown ='ignore', max_categories = 4), ['Pclass']),\n",
    "    ('embarked', preprocessing.OneHotEncoder(handle_unknown ='ignore', max_categories = 3), ['Embarked']),\n",
    "    ('family', preprocessing.OneHotEncoder(drop='if_binary', handle_unknown ='ignore', max_categories = 2), ['SibSp','Parch']),\n",
    "    ('fare', preprocessing.KBinsDiscretizer(n_bins= 5, encode='ordinal', strategy='quantile'), ['Fare']),\n",
    "    ('age',  preprocessing.FunctionTransformer(bin_values), ['Age'])\n",
    "    ], remainder='drop')\n",
    "\n",
    "# Transform tthe train and test data\n",
    "\n",
    "x_train_transformed = column_trans.fit_transform(x_train)\n",
    "x_test_transformed = column_trans.fit_transform(x_test)\n",
    "\n",
    "# The list of classifiers that we are going to try. The parameters have been chosen after playing a bit with them.\n",
    "\n",
    "clf1 = tree.DecisionTreeClassifier(criterion='entropy', min_samples_leaf = 15)\n",
    "clf2 = ensemble.GradientBoostingClassifier()\n",
    "clf3 = naive_bayes.CategoricalNB()\n",
    "clf4 = neighbors.KNeighborsClassifier(n_neighbors= 50)\n",
    "clf5 = svm.SVC(kernel='poly')\n",
    "clf6 = ensemble.RandomForestClassifier(criterion='entropy', min_samples_leaf = 15)\n",
    "clf7 = linear_model.SGDClassifier()\n",
    "\n",
    "# Training each model, and obtaining the accuracies in the train as well as test data sets.\n",
    "\n",
    "for clf, label in zip([clf1,clf2,clf3,clf4,clf5,clf6,clf7],['Tree','Grad. Boost','GaussianNB','K Neighnors', 'SVC', 'Random Forrest', 'SGD']):\n",
    "    clf.fit(x_train_transformed,y_train)\n",
    "    y_train_pred = clf.predict(x_train_transformed)\n",
    "    y_test_pred = clf.predict(x_test_transformed)\n",
    "    print(\"Accuracy: %0.4f  [%s]\" % (metrics.accuracy_score(y_train,y_train_pred), 'Train set accuracy '+ label))\n",
    "    print(\"Accuracy: %0.4f  [%s]\" % (metrics.accuracy_score(y_test,y_test_pred), 'Test set accuracy '+ label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f114e7f",
   "metadata": {},
   "source": [
    "We obtain fairly similar results with the various models. The tree classifier is still very competitive once we set a good value of min_samples_leaf. Random forrest doesn't perform better than a single Decision tree, probably because the numer of features is limited. Gradient boost on the other hand does as good as a single decision tree. The best model, however, is the Support Vector Machine with the degree 3 polynomial kernel. It also shows less variance than the tree based models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d863ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7894736842105263"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We print the SVC results to submmit at Kaggle.\n",
    "\n",
    "clf5.fit(x_train_transformed,y_train)\n",
    "y_test_pred = clf5.predict(x_test_transformed)\n",
    "test['Survived']= y_test_pred\n",
    "test[['PassengerId','Survived']].to_csv('submission.csv',index=False)\n",
    "metrics.accuracy_score(y_test,y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
